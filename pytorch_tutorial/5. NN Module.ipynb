{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd31fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a43847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08136f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['id','Unnamed: 32'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cf4671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(df.drop('diagnosis',axis=1),df['diagnosis'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de3cbc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "614580d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=LabelEncoder()\n",
    "y_train=encoder.fit_transform(y_train)\n",
    "y_test=encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f0ac0b",
   "metadata": {},
   "source": [
    "## Numpy to Torch Tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe512846",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=torch.from_numpy(X_train)\n",
    "X_test=torch.from_numpy(X_test)\n",
    "y_train=torch.from_numpy(y_train)\n",
    "y_test=torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1a6d17e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.1\n",
    "epochs=25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5459441c",
   "metadata": {},
   "source": [
    "### Using `torch.nn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8ea75fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7bb36b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(num_features, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, features):\n",
    "        # Convert input to float32 to match weight types\n",
    "        features = features.float() \n",
    "        \n",
    "        out = self.linear(features)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e641aef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([455, 30])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76c4b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fb889ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5865],\n",
       "        [0.4753],\n",
       "        [0.4262],\n",
       "        [0.6249],\n",
       "        [0.6051],\n",
       "        [0.5326],\n",
       "        [0.6176],\n",
       "        [0.4350],\n",
       "        [0.4686],\n",
       "        [0.3630],\n",
       "        [0.5393],\n",
       "        [0.4013],\n",
       "        [0.5696],\n",
       "        [0.5648],\n",
       "        [0.5095],\n",
       "        [0.4109],\n",
       "        [0.3651],\n",
       "        [0.6190],\n",
       "        [0.3976],\n",
       "        [0.5412],\n",
       "        [0.5281],\n",
       "        [0.5906],\n",
       "        [0.6262],\n",
       "        [0.5673],\n",
       "        [0.3453],\n",
       "        [0.3282],\n",
       "        [0.3676],\n",
       "        [0.7577],\n",
       "        [0.6714],\n",
       "        [0.7344],\n",
       "        [0.5753],\n",
       "        [0.5991],\n",
       "        [0.4435],\n",
       "        [0.4428],\n",
       "        [0.6392],\n",
       "        [0.4331],\n",
       "        [0.5663],\n",
       "        [0.5482],\n",
       "        [0.5600],\n",
       "        [0.6923],\n",
       "        [0.6163],\n",
       "        [0.5222],\n",
       "        [0.5667],\n",
       "        [0.5347],\n",
       "        [0.4105],\n",
       "        [0.2521],\n",
       "        [0.3538],\n",
       "        [0.2950],\n",
       "        [0.3976],\n",
       "        [0.4589],\n",
       "        [0.5892],\n",
       "        [0.4601],\n",
       "        [0.4318],\n",
       "        [0.3575],\n",
       "        [0.7513],\n",
       "        [0.5058],\n",
       "        [0.6837],\n",
       "        [0.3554],\n",
       "        [0.4794],\n",
       "        [0.7086],\n",
       "        [0.5913],\n",
       "        [0.4941],\n",
       "        [0.6757],\n",
       "        [0.4904],\n",
       "        [0.6921],\n",
       "        [0.5430],\n",
       "        [0.2814],\n",
       "        [0.3994],\n",
       "        [0.5159],\n",
       "        [0.6728],\n",
       "        [0.4953],\n",
       "        [0.4677],\n",
       "        [0.3427],\n",
       "        [0.3768],\n",
       "        [0.5135],\n",
       "        [0.7470],\n",
       "        [0.4892],\n",
       "        [0.4899],\n",
       "        [0.6503],\n",
       "        [0.2676],\n",
       "        [0.4115],\n",
       "        [0.6078],\n",
       "        [0.5809],\n",
       "        [0.6503],\n",
       "        [0.4138],\n",
       "        [0.5359],\n",
       "        [0.7458],\n",
       "        [0.5420],\n",
       "        [0.5262],\n",
       "        [0.6106],\n",
       "        [0.5330],\n",
       "        [0.4469],\n",
       "        [0.2887],\n",
       "        [0.5280],\n",
       "        [0.5940],\n",
       "        [0.5388],\n",
       "        [0.4102],\n",
       "        [0.5482],\n",
       "        [0.5260],\n",
       "        [0.6010],\n",
       "        [0.6279],\n",
       "        [0.3597],\n",
       "        [0.2486],\n",
       "        [0.5533],\n",
       "        [0.6428],\n",
       "        [0.5218],\n",
       "        [0.5024],\n",
       "        [0.4515],\n",
       "        [0.6330],\n",
       "        [0.5698],\n",
       "        [0.5316],\n",
       "        [0.4926],\n",
       "        [0.3121],\n",
       "        [0.5986],\n",
       "        [0.5297],\n",
       "        [0.4823],\n",
       "        [0.5120],\n",
       "        [0.7859],\n",
       "        [0.3409],\n",
       "        [0.4631],\n",
       "        [0.6182],\n",
       "        [0.4945],\n",
       "        [0.6578],\n",
       "        [0.4579],\n",
       "        [0.6850],\n",
       "        [0.4011],\n",
       "        [0.6371],\n",
       "        [0.6007],\n",
       "        [0.6113],\n",
       "        [0.4799],\n",
       "        [0.3449],\n",
       "        [0.5830],\n",
       "        [0.5310],\n",
       "        [0.3444],\n",
       "        [0.3769],\n",
       "        [0.7814],\n",
       "        [0.6246],\n",
       "        [0.4841],\n",
       "        [0.3560],\n",
       "        [0.4875],\n",
       "        [0.6179],\n",
       "        [0.5541],\n",
       "        [0.4962],\n",
       "        [0.3497],\n",
       "        [0.5149],\n",
       "        [0.7427],\n",
       "        [0.4699],\n",
       "        [0.3969],\n",
       "        [0.6643],\n",
       "        [0.5716],\n",
       "        [0.4570],\n",
       "        [0.7079],\n",
       "        [0.5357],\n",
       "        [0.4975],\n",
       "        [0.4781],\n",
       "        [0.3552],\n",
       "        [0.5260],\n",
       "        [0.6833],\n",
       "        [0.6034],\n",
       "        [0.4529],\n",
       "        [0.6323],\n",
       "        [0.7369],\n",
       "        [0.4423],\n",
       "        [0.5937],\n",
       "        [0.5909],\n",
       "        [0.5462],\n",
       "        [0.5381],\n",
       "        [0.3718],\n",
       "        [0.5184],\n",
       "        [0.4996],\n",
       "        [0.7608],\n",
       "        [0.6956],\n",
       "        [0.6831],\n",
       "        [0.7137],\n",
       "        [0.6690],\n",
       "        [0.6297],\n",
       "        [0.4305],\n",
       "        [0.6510],\n",
       "        [0.5343],\n",
       "        [0.4463],\n",
       "        [0.6628],\n",
       "        [0.7026],\n",
       "        [0.4955],\n",
       "        [0.4076],\n",
       "        [0.7260],\n",
       "        [0.6799],\n",
       "        [0.6564],\n",
       "        [0.6013],\n",
       "        [0.5435],\n",
       "        [0.6726],\n",
       "        [0.5585],\n",
       "        [0.6386],\n",
       "        [0.4762],\n",
       "        [0.6927],\n",
       "        [0.5252],\n",
       "        [0.6150],\n",
       "        [0.7798],\n",
       "        [0.5405],\n",
       "        [0.6748],\n",
       "        [0.6279],\n",
       "        [0.4086],\n",
       "        [0.2933],\n",
       "        [0.3515],\n",
       "        [0.5473],\n",
       "        [0.4417],\n",
       "        [0.4092],\n",
       "        [0.6008],\n",
       "        [0.5377],\n",
       "        [0.5350],\n",
       "        [0.4644],\n",
       "        [0.5753],\n",
       "        [0.7146],\n",
       "        [0.4956],\n",
       "        [0.5943],\n",
       "        [0.5734],\n",
       "        [0.5388],\n",
       "        [0.5618],\n",
       "        [0.6474],\n",
       "        [0.6328],\n",
       "        [0.5052],\n",
       "        [0.4761],\n",
       "        [0.6110],\n",
       "        [0.5082],\n",
       "        [0.5772],\n",
       "        [0.5132],\n",
       "        [0.5792],\n",
       "        [0.5164],\n",
       "        [0.4203],\n",
       "        [0.6388],\n",
       "        [0.3247],\n",
       "        [0.4200],\n",
       "        [0.6411],\n",
       "        [0.4929],\n",
       "        [0.6094],\n",
       "        [0.8259],\n",
       "        [0.6041],\n",
       "        [0.6618],\n",
       "        [0.6479],\n",
       "        [0.2407],\n",
       "        [0.4663],\n",
       "        [0.5480],\n",
       "        [0.4758],\n",
       "        [0.6157],\n",
       "        [0.3216],\n",
       "        [0.6930],\n",
       "        [0.5445],\n",
       "        [0.3929],\n",
       "        [0.5648],\n",
       "        [0.5353],\n",
       "        [0.2590],\n",
       "        [0.4770],\n",
       "        [0.6523],\n",
       "        [0.7059],\n",
       "        [0.4903],\n",
       "        [0.8316],\n",
       "        [0.2478],\n",
       "        [0.4898],\n",
       "        [0.8650],\n",
       "        [0.5173],\n",
       "        [0.6512],\n",
       "        [0.4869],\n",
       "        [0.5795],\n",
       "        [0.5554],\n",
       "        [0.5110],\n",
       "        [0.7201],\n",
       "        [0.3122],\n",
       "        [0.4863],\n",
       "        [0.5509],\n",
       "        [0.6264],\n",
       "        [0.4017],\n",
       "        [0.2799],\n",
       "        [0.6993],\n",
       "        [0.4091],\n",
       "        [0.4452],\n",
       "        [0.5352],\n",
       "        [0.6601],\n",
       "        [0.4184],\n",
       "        [0.2715],\n",
       "        [0.5330],\n",
       "        [0.6140],\n",
       "        [0.4763],\n",
       "        [0.1346],\n",
       "        [0.5857],\n",
       "        [0.8152],\n",
       "        [0.5742],\n",
       "        [0.6492],\n",
       "        [0.4392],\n",
       "        [0.4117],\n",
       "        [0.5810],\n",
       "        [0.4758],\n",
       "        [0.6734],\n",
       "        [0.8264],\n",
       "        [0.2505],\n",
       "        [0.7049],\n",
       "        [0.3864],\n",
       "        [0.4242],\n",
       "        [0.6557],\n",
       "        [0.5880],\n",
       "        [0.6854],\n",
       "        [0.5846],\n",
       "        [0.4588],\n",
       "        [0.5326],\n",
       "        [0.3484],\n",
       "        [0.6339],\n",
       "        [0.5555],\n",
       "        [0.4848],\n",
       "        [0.6575],\n",
       "        [0.6067],\n",
       "        [0.6013],\n",
       "        [0.6689],\n",
       "        [0.5444],\n",
       "        [0.6891],\n",
       "        [0.5272],\n",
       "        [0.4063],\n",
       "        [0.4731],\n",
       "        [0.6058],\n",
       "        [0.5475],\n",
       "        [0.4679],\n",
       "        [0.6799],\n",
       "        [0.6302],\n",
       "        [0.7400],\n",
       "        [0.5655],\n",
       "        [0.5540],\n",
       "        [0.4569],\n",
       "        [0.3859],\n",
       "        [0.6073],\n",
       "        [0.5410],\n",
       "        [0.6729],\n",
       "        [0.6872],\n",
       "        [0.4720],\n",
       "        [0.5384],\n",
       "        [0.5134],\n",
       "        [0.5714],\n",
       "        [0.3999],\n",
       "        [0.5474],\n",
       "        [0.5700],\n",
       "        [0.5771],\n",
       "        [0.4988],\n",
       "        [0.3918],\n",
       "        [0.4671],\n",
       "        [0.5028],\n",
       "        [0.4601],\n",
       "        [0.4207],\n",
       "        [0.2748],\n",
       "        [0.4067],\n",
       "        [0.6480],\n",
       "        [0.5039],\n",
       "        [0.4019],\n",
       "        [0.4219],\n",
       "        [0.6164],\n",
       "        [0.6429],\n",
       "        [0.4647],\n",
       "        [0.4068],\n",
       "        [0.5037],\n",
       "        [0.6761],\n",
       "        [0.6712],\n",
       "        [0.5580],\n",
       "        [0.4494],\n",
       "        [0.5544],\n",
       "        [0.7702],\n",
       "        [0.4958],\n",
       "        [0.4789],\n",
       "        [0.3473],\n",
       "        [0.3462],\n",
       "        [0.5768],\n",
       "        [0.6102],\n",
       "        [0.4751],\n",
       "        [0.4085],\n",
       "        [0.6147],\n",
       "        [0.4762],\n",
       "        [0.5775],\n",
       "        [0.4685],\n",
       "        [0.4496],\n",
       "        [0.7110],\n",
       "        [0.5638],\n",
       "        [0.5089],\n",
       "        [0.6648],\n",
       "        [0.4923],\n",
       "        [0.6183],\n",
       "        [0.3873],\n",
       "        [0.5577],\n",
       "        [0.5621],\n",
       "        [0.6220],\n",
       "        [0.5513],\n",
       "        [0.4639],\n",
       "        [0.5487],\n",
       "        [0.6684],\n",
       "        [0.3961],\n",
       "        [0.5843],\n",
       "        [0.5893],\n",
       "        [0.3012],\n",
       "        [0.6405],\n",
       "        [0.4152],\n",
       "        [0.3861],\n",
       "        [0.5472],\n",
       "        [0.4268],\n",
       "        [0.5297],\n",
       "        [0.5429],\n",
       "        [0.3687],\n",
       "        [0.7068],\n",
       "        [0.5522],\n",
       "        [0.4249],\n",
       "        [0.8531],\n",
       "        [0.4888],\n",
       "        [0.7517],\n",
       "        [0.4120],\n",
       "        [0.3315],\n",
       "        [0.7820],\n",
       "        [0.3943],\n",
       "        [0.5885],\n",
       "        [0.5846],\n",
       "        [0.5499],\n",
       "        [0.5326],\n",
       "        [0.6177],\n",
       "        [0.5468],\n",
       "        [0.3659],\n",
       "        [0.7830],\n",
       "        [0.5527],\n",
       "        [0.4777],\n",
       "        [0.4280],\n",
       "        [0.5004],\n",
       "        [0.5820],\n",
       "        [0.2472],\n",
       "        [0.3795],\n",
       "        [0.3867],\n",
       "        [0.3339],\n",
       "        [0.5860],\n",
       "        [0.6507],\n",
       "        [0.5007],\n",
       "        [0.5918],\n",
       "        [0.5213],\n",
       "        [0.8806],\n",
       "        [0.7180],\n",
       "        [0.5439],\n",
       "        [0.4674],\n",
       "        [0.3362],\n",
       "        [0.6065],\n",
       "        [0.6752],\n",
       "        [0.5781],\n",
       "        [0.5685],\n",
       "        [0.4287],\n",
       "        [0.6305],\n",
       "        [0.4838],\n",
       "        [0.6703],\n",
       "        [0.5407],\n",
       "        [0.4667],\n",
       "        [0.4537],\n",
       "        [0.5002],\n",
       "        [0.4741],\n",
       "        [0.5006],\n",
       "        [0.4634],\n",
       "        [0.6363],\n",
       "        [0.4438],\n",
       "        [0.6666],\n",
       "        [0.4093]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0837cd38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.1902e-01, -3.7342e-05, -1.5269e-01,  7.4766e-03,  1.4983e-01,\n",
       "         -1.0595e-01, -1.4289e-02,  5.0171e-02,  1.7530e-01,  1.5977e-01,\n",
       "          6.0674e-02, -5.9230e-03,  1.1697e-02,  1.7400e-01, -1.1913e-01,\n",
       "         -2.0301e-02,  3.0888e-02, -1.6609e-01,  1.6682e-01,  1.1478e-02,\n",
       "         -5.6829e-02,  2.8467e-02, -1.5790e-01, -1.5657e-01,  1.0765e-02,\n",
       "         -1.0896e-01, -8.5537e-02, -1.2240e-02,  3.0942e-02,  1.9072e-02]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9aedf5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.1408], requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0dec602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "000543ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model                                    [455, 1]                  --\n",
       "├─Linear: 1-1                            [455, 1]                  31\n",
       "├─Sigmoid: 1-2                           [455, 1]                  --\n",
       "==========================================================================================\n",
       "Total params: 31\n",
       "Trainable params: 31\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.01\n",
       "==========================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.06\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model,input_size=X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8f48f0",
   "metadata": {},
   "source": [
    "### <br>With Hidden Layer</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b488a099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_2(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(num_features, 60)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear_2=nn.Linear(60,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, features):\n",
    "        # Convert input to float32 to match weight types\n",
    "        features = features.float() \n",
    "        \n",
    "        out = self.linear_1(features)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear_2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6df97d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model_2(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1d5348d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4770],\n",
       "        [0.5078],\n",
       "        [0.5427],\n",
       "        [0.3625],\n",
       "        [0.4847],\n",
       "        [0.5163],\n",
       "        [0.3997],\n",
       "        [0.5009],\n",
       "        [0.5280],\n",
       "        [0.5210],\n",
       "        [0.4688],\n",
       "        [0.5130],\n",
       "        [0.4022],\n",
       "        [0.4936],\n",
       "        [0.5002],\n",
       "        [0.5322],\n",
       "        [0.4474],\n",
       "        [0.4640],\n",
       "        [0.5590],\n",
       "        [0.4730],\n",
       "        [0.4656],\n",
       "        [0.4443],\n",
       "        [0.4883],\n",
       "        [0.3794],\n",
       "        [0.4381],\n",
       "        [0.5001],\n",
       "        [0.4527],\n",
       "        [0.4952],\n",
       "        [0.4700],\n",
       "        [0.4878],\n",
       "        [0.5079],\n",
       "        [0.4652],\n",
       "        [0.5478],\n",
       "        [0.4487],\n",
       "        [0.4526],\n",
       "        [0.5201],\n",
       "        [0.5132],\n",
       "        [0.4690],\n",
       "        [0.5015],\n",
       "        [0.3708],\n",
       "        [0.4272],\n",
       "        [0.4311],\n",
       "        [0.5088],\n",
       "        [0.4775],\n",
       "        [0.5486],\n",
       "        [0.4869],\n",
       "        [0.5661],\n",
       "        [0.4860],\n",
       "        [0.5359],\n",
       "        [0.5588],\n",
       "        [0.5292],\n",
       "        [0.4679],\n",
       "        [0.5306],\n",
       "        [0.4279],\n",
       "        [0.4771],\n",
       "        [0.5278],\n",
       "        [0.3921],\n",
       "        [0.5209],\n",
       "        [0.4426],\n",
       "        [0.3803],\n",
       "        [0.5183],\n",
       "        [0.4982],\n",
       "        [0.5201],\n",
       "        [0.4973],\n",
       "        [0.4350],\n",
       "        [0.5239],\n",
       "        [0.5731],\n",
       "        [0.4131],\n",
       "        [0.4871],\n",
       "        [0.4705],\n",
       "        [0.4738],\n",
       "        [0.5423],\n",
       "        [0.5401],\n",
       "        [0.4527],\n",
       "        [0.4355],\n",
       "        [0.4666],\n",
       "        [0.5210],\n",
       "        [0.3958],\n",
       "        [0.4478],\n",
       "        [0.5482],\n",
       "        [0.4271],\n",
       "        [0.4799],\n",
       "        [0.5182],\n",
       "        [0.4718],\n",
       "        [0.4993],\n",
       "        [0.4477],\n",
       "        [0.3512],\n",
       "        [0.4994],\n",
       "        [0.4610],\n",
       "        [0.4517],\n",
       "        [0.5009],\n",
       "        [0.5109],\n",
       "        [0.5610],\n",
       "        [0.4696],\n",
       "        [0.4987],\n",
       "        [0.5431],\n",
       "        [0.5471],\n",
       "        [0.4593],\n",
       "        [0.4391],\n",
       "        [0.4348],\n",
       "        [0.4575],\n",
       "        [0.5215],\n",
       "        [0.5515],\n",
       "        [0.4795],\n",
       "        [0.4948],\n",
       "        [0.5352],\n",
       "        [0.5201],\n",
       "        [0.5375],\n",
       "        [0.4164],\n",
       "        [0.5008],\n",
       "        [0.5005],\n",
       "        [0.5112],\n",
       "        [0.5482],\n",
       "        [0.4729],\n",
       "        [0.5051],\n",
       "        [0.4988],\n",
       "        [0.5287],\n",
       "        [0.4079],\n",
       "        [0.4707],\n",
       "        [0.4697],\n",
       "        [0.4663],\n",
       "        [0.5297],\n",
       "        [0.4851],\n",
       "        [0.4491],\n",
       "        [0.3290],\n",
       "        [0.5341],\n",
       "        [0.4186],\n",
       "        [0.5122],\n",
       "        [0.5160],\n",
       "        [0.5159],\n",
       "        [0.5093],\n",
       "        [0.4177],\n",
       "        [0.5162],\n",
       "        [0.5191],\n",
       "        [0.5412],\n",
       "        [0.4208],\n",
       "        [0.5184],\n",
       "        [0.4026],\n",
       "        [0.5220],\n",
       "        [0.5504],\n",
       "        [0.4714],\n",
       "        [0.4428],\n",
       "        [0.5112],\n",
       "        [0.4928],\n",
       "        [0.4897],\n",
       "        [0.4354],\n",
       "        [0.5177],\n",
       "        [0.5014],\n",
       "        [0.4787],\n",
       "        [0.4673],\n",
       "        [0.4905],\n",
       "        [0.4978],\n",
       "        [0.5016],\n",
       "        [0.5042],\n",
       "        [0.4718],\n",
       "        [0.5425],\n",
       "        [0.4807],\n",
       "        [0.4598],\n",
       "        [0.4555],\n",
       "        [0.5168],\n",
       "        [0.4755],\n",
       "        [0.3885],\n",
       "        [0.4829],\n",
       "        [0.5024],\n",
       "        [0.5217],\n",
       "        [0.4672],\n",
       "        [0.4847],\n",
       "        [0.5323],\n",
       "        [0.5238],\n",
       "        [0.4430],\n",
       "        [0.4811],\n",
       "        [0.4859],\n",
       "        [0.4683],\n",
       "        [0.4378],\n",
       "        [0.4159],\n",
       "        [0.3807],\n",
       "        [0.4891],\n",
       "        [0.4484],\n",
       "        [0.4919],\n",
       "        [0.4814],\n",
       "        [0.4851],\n",
       "        [0.4564],\n",
       "        [0.5488],\n",
       "        [0.4626],\n",
       "        [0.2848],\n",
       "        [0.5356],\n",
       "        [0.5091],\n",
       "        [0.4600],\n",
       "        [0.4717],\n",
       "        [0.3352],\n",
       "        [0.3822],\n",
       "        [0.3415],\n",
       "        [0.4649],\n",
       "        [0.4193],\n",
       "        [0.3449],\n",
       "        [0.4356],\n",
       "        [0.4797],\n",
       "        [0.5003],\n",
       "        [0.3841],\n",
       "        [0.4513],\n",
       "        [0.5630],\n",
       "        [0.4478],\n",
       "        [0.5693],\n",
       "        [0.5072],\n",
       "        [0.5371],\n",
       "        [0.5450],\n",
       "        [0.4085],\n",
       "        [0.5024],\n",
       "        [0.5278],\n",
       "        [0.3786],\n",
       "        [0.4357],\n",
       "        [0.4229],\n",
       "        [0.4506],\n",
       "        [0.4484],\n",
       "        [0.5341],\n",
       "        [0.4523],\n",
       "        [0.4615],\n",
       "        [0.4420],\n",
       "        [0.3934],\n",
       "        [0.5094],\n",
       "        [0.4894],\n",
       "        [0.4408],\n",
       "        [0.5247],\n",
       "        [0.5020],\n",
       "        [0.5358],\n",
       "        [0.4747],\n",
       "        [0.5031],\n",
       "        [0.4366],\n",
       "        [0.4181],\n",
       "        [0.5333],\n",
       "        [0.4932],\n",
       "        [0.4445],\n",
       "        [0.5160],\n",
       "        [0.5103],\n",
       "        [0.4032],\n",
       "        [0.4504],\n",
       "        [0.4380],\n",
       "        [0.4419],\n",
       "        [0.5783],\n",
       "        [0.4624],\n",
       "        [0.4562],\n",
       "        [0.5527],\n",
       "        [0.5214],\n",
       "        [0.5229],\n",
       "        [0.4505],\n",
       "        [0.5358],\n",
       "        [0.5563],\n",
       "        [0.5422],\n",
       "        [0.5106],\n",
       "        [0.4551],\n",
       "        [0.4717],\n",
       "        [0.3477],\n",
       "        [0.2554],\n",
       "        [0.5084],\n",
       "        [0.3916],\n",
       "        [0.4646],\n",
       "        [0.5229],\n",
       "        [0.2707],\n",
       "        [0.5149],\n",
       "        [0.4193],\n",
       "        [0.5208],\n",
       "        [0.3765],\n",
       "        [0.5003],\n",
       "        [0.4127],\n",
       "        [0.3310],\n",
       "        [0.4782],\n",
       "        [0.4813],\n",
       "        [0.3976],\n",
       "        [0.4437],\n",
       "        [0.5538],\n",
       "        [0.5254],\n",
       "        [0.4352],\n",
       "        [0.5023],\n",
       "        [0.5265],\n",
       "        [0.5456],\n",
       "        [0.4336],\n",
       "        [0.5272],\n",
       "        [0.5065],\n",
       "        [0.4375],\n",
       "        [0.5085],\n",
       "        [0.5271],\n",
       "        [0.4147],\n",
       "        [0.4885],\n",
       "        [0.2796],\n",
       "        [0.4793],\n",
       "        [0.4383],\n",
       "        [0.4709],\n",
       "        [0.3997],\n",
       "        [0.4818],\n",
       "        [0.5153],\n",
       "        [0.4284],\n",
       "        [0.3207],\n",
       "        [0.5432],\n",
       "        [0.4085],\n",
       "        [0.4753],\n",
       "        [0.5404],\n",
       "        [0.3867],\n",
       "        [0.4422],\n",
       "        [0.4272],\n",
       "        [0.4877],\n",
       "        [0.5534],\n",
       "        [0.5172],\n",
       "        [0.5125],\n",
       "        [0.5036],\n",
       "        [0.4206],\n",
       "        [0.4629],\n",
       "        [0.4723],\n",
       "        [0.4558],\n",
       "        [0.4660],\n",
       "        [0.4517],\n",
       "        [0.5073],\n",
       "        [0.3874],\n",
       "        [0.5090],\n",
       "        [0.5276],\n",
       "        [0.5128],\n",
       "        [0.4330],\n",
       "        [0.4298],\n",
       "        [0.5679],\n",
       "        [0.3795],\n",
       "        [0.4718],\n",
       "        [0.5119],\n",
       "        [0.5302],\n",
       "        [0.4948],\n",
       "        [0.5291],\n",
       "        [0.4973],\n",
       "        [0.5239],\n",
       "        [0.4981],\n",
       "        [0.4612],\n",
       "        [0.4592],\n",
       "        [0.5521],\n",
       "        [0.4924],\n",
       "        [0.5348],\n",
       "        [0.4481],\n",
       "        [0.5363],\n",
       "        [0.4932],\n",
       "        [0.5135],\n",
       "        [0.5097],\n",
       "        [0.5113],\n",
       "        [0.3892],\n",
       "        [0.5505],\n",
       "        [0.5565],\n",
       "        [0.5158],\n",
       "        [0.5153],\n",
       "        [0.5488],\n",
       "        [0.5427],\n",
       "        [0.4598],\n",
       "        [0.4994],\n",
       "        [0.5156],\n",
       "        [0.4437],\n",
       "        [0.4804],\n",
       "        [0.5046],\n",
       "        [0.5170],\n",
       "        [0.5308],\n",
       "        [0.5144],\n",
       "        [0.4666],\n",
       "        [0.4253],\n",
       "        [0.4964],\n",
       "        [0.5645],\n",
       "        [0.5437],\n",
       "        [0.3602],\n",
       "        [0.5343],\n",
       "        [0.4600],\n",
       "        [0.4697],\n",
       "        [0.5718],\n",
       "        [0.5008],\n",
       "        [0.4390],\n",
       "        [0.5670],\n",
       "        [0.5144],\n",
       "        [0.3500],\n",
       "        [0.5014],\n",
       "        [0.5407],\n",
       "        [0.4941],\n",
       "        [0.4779],\n",
       "        [0.4843],\n",
       "        [0.5061],\n",
       "        [0.5199],\n",
       "        [0.3926],\n",
       "        [0.4809],\n",
       "        [0.4766],\n",
       "        [0.4929],\n",
       "        [0.4674],\n",
       "        [0.4850],\n",
       "        [0.4004],\n",
       "        [0.5208],\n",
       "        [0.4333],\n",
       "        [0.4850],\n",
       "        [0.4859],\n",
       "        [0.5023],\n",
       "        [0.4677],\n",
       "        [0.3444],\n",
       "        [0.5185],\n",
       "        [0.5063],\n",
       "        [0.5571],\n",
       "        [0.4991],\n",
       "        [0.5142],\n",
       "        [0.4836],\n",
       "        [0.3806],\n",
       "        [0.4486],\n",
       "        [0.5125],\n",
       "        [0.3453],\n",
       "        [0.5444],\n",
       "        [0.5661],\n",
       "        [0.2126],\n",
       "        [0.5108],\n",
       "        [0.4268],\n",
       "        [0.5484],\n",
       "        [0.5405],\n",
       "        [0.2841],\n",
       "        [0.5035],\n",
       "        [0.4263],\n",
       "        [0.4222],\n",
       "        [0.4621],\n",
       "        [0.5409],\n",
       "        [0.5227],\n",
       "        [0.4719],\n",
       "        [0.5870],\n",
       "        [0.4565],\n",
       "        [0.4009],\n",
       "        [0.5525],\n",
       "        [0.4807],\n",
       "        [0.4900],\n",
       "        [0.4938],\n",
       "        [0.5687],\n",
       "        [0.5201],\n",
       "        [0.5244],\n",
       "        [0.4573],\n",
       "        [0.4451],\n",
       "        [0.4806],\n",
       "        [0.5147],\n",
       "        [0.5206],\n",
       "        [0.4795],\n",
       "        [0.3348],\n",
       "        [0.4947],\n",
       "        [0.4824],\n",
       "        [0.5213],\n",
       "        [0.5511],\n",
       "        [0.4956],\n",
       "        [0.4783],\n",
       "        [0.4937],\n",
       "        [0.3870],\n",
       "        [0.4252],\n",
       "        [0.3655],\n",
       "        [0.4632],\n",
       "        [0.4194],\n",
       "        [0.4525],\n",
       "        [0.5221],\n",
       "        [0.5459],\n",
       "        [0.5009],\n",
       "        [0.5069],\n",
       "        [0.5089],\n",
       "        [0.4496],\n",
       "        [0.4878],\n",
       "        [0.4803],\n",
       "        [0.4241],\n",
       "        [0.5560]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5670596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model_2                                  [455, 1]                  --\n",
       "├─Linear: 1-1                            [455, 60]                 1,860\n",
       "├─ReLU: 1-2                              [455, 60]                 --\n",
       "├─Linear: 1-3                            [455, 1]                  61\n",
       "├─Sigmoid: 1-4                           [455, 1]                  --\n",
       "==========================================================================================\n",
       "Total params: 1,921\n",
       "Trainable params: 1,921\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.87\n",
       "==========================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 0.22\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.28\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=model,input_size=X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83484c8e",
   "metadata": {},
   "source": [
    "### Using `Sequential`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52ea82ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_3(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.network=nn.Sequential(\n",
    "        nn.Linear(num_features, 60),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(60,1),\n",
    "        nn.Sigmoid()\n",
    "        )\n",
    "       \n",
    "\n",
    "    def forward(self, features):\n",
    "        # Convert input to float32 to match weight types\n",
    "        features = features.float() \n",
    "        out = self.network(features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5aaaa7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model_3(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3c0990b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5221],\n",
       "        [0.5604],\n",
       "        [0.4930],\n",
       "        [0.5748],\n",
       "        [0.5494],\n",
       "        [0.5401],\n",
       "        [0.5686],\n",
       "        [0.3962],\n",
       "        [0.4761],\n",
       "        [0.4933],\n",
       "        [0.4413],\n",
       "        [0.5172],\n",
       "        [0.5342],\n",
       "        [0.4859],\n",
       "        [0.4704],\n",
       "        [0.5247],\n",
       "        [0.4958],\n",
       "        [0.5857],\n",
       "        [0.4737],\n",
       "        [0.4896],\n",
       "        [0.5547],\n",
       "        [0.5497],\n",
       "        [0.4927],\n",
       "        [0.4740],\n",
       "        [0.5309],\n",
       "        [0.4940],\n",
       "        [0.4488],\n",
       "        [0.5358],\n",
       "        [0.5545],\n",
       "        [0.5805],\n",
       "        [0.5311],\n",
       "        [0.4879],\n",
       "        [0.4982],\n",
       "        [0.5814],\n",
       "        [0.5373],\n",
       "        [0.5375],\n",
       "        [0.5548],\n",
       "        [0.4975],\n",
       "        [0.5420],\n",
       "        [0.5163],\n",
       "        [0.5264],\n",
       "        [0.5013],\n",
       "        [0.5482],\n",
       "        [0.5216],\n",
       "        [0.4420],\n",
       "        [0.3948],\n",
       "        [0.4135],\n",
       "        [0.4640],\n",
       "        [0.5273],\n",
       "        [0.5275],\n",
       "        [0.5531],\n",
       "        [0.5104],\n",
       "        [0.5461],\n",
       "        [0.5989],\n",
       "        [0.5569],\n",
       "        [0.5048],\n",
       "        [0.5613],\n",
       "        [0.4817],\n",
       "        [0.5915],\n",
       "        [0.5352],\n",
       "        [0.5439],\n",
       "        [0.5194],\n",
       "        [0.5610],\n",
       "        [0.4686],\n",
       "        [0.5565],\n",
       "        [0.5177],\n",
       "        [0.4740],\n",
       "        [0.5845],\n",
       "        [0.5454],\n",
       "        [0.5304],\n",
       "        [0.5042],\n",
       "        [0.4343],\n",
       "        [0.4751],\n",
       "        [0.4086],\n",
       "        [0.5687],\n",
       "        [0.5821],\n",
       "        [0.4432],\n",
       "        [0.4461],\n",
       "        [0.5307],\n",
       "        [0.4851],\n",
       "        [0.3942],\n",
       "        [0.5266],\n",
       "        [0.5318],\n",
       "        [0.5278],\n",
       "        [0.5136],\n",
       "        [0.5182],\n",
       "        [0.5577],\n",
       "        [0.5518],\n",
       "        [0.5920],\n",
       "        [0.5706],\n",
       "        [0.5500],\n",
       "        [0.5469],\n",
       "        [0.4716],\n",
       "        [0.5386],\n",
       "        [0.5782],\n",
       "        [0.5322],\n",
       "        [0.5084],\n",
       "        [0.5087],\n",
       "        [0.5425],\n",
       "        [0.5784],\n",
       "        [0.5492],\n",
       "        [0.4518],\n",
       "        [0.5114],\n",
       "        [0.5467],\n",
       "        [0.5433],\n",
       "        [0.5085],\n",
       "        [0.5518],\n",
       "        [0.5108],\n",
       "        [0.4721],\n",
       "        [0.5782],\n",
       "        [0.5071],\n",
       "        [0.5452],\n",
       "        [0.4023],\n",
       "        [0.5561],\n",
       "        [0.4930],\n",
       "        [0.4495],\n",
       "        [0.4510],\n",
       "        [0.5564],\n",
       "        [0.5737],\n",
       "        [0.5204],\n",
       "        [0.5570],\n",
       "        [0.5052],\n",
       "        [0.5398],\n",
       "        [0.4799],\n",
       "        [0.4412],\n",
       "        [0.4772],\n",
       "        [0.4675],\n",
       "        [0.4743],\n",
       "        [0.5667],\n",
       "        [0.5255],\n",
       "        [0.5428],\n",
       "        [0.6139],\n",
       "        [0.5545],\n",
       "        [0.4390],\n",
       "        [0.5182],\n",
       "        [0.6002],\n",
       "        [0.5371],\n",
       "        [0.5766],\n",
       "        [0.5441],\n",
       "        [0.5418],\n",
       "        [0.5008],\n",
       "        [0.4038],\n",
       "        [0.4998],\n",
       "        [0.4701],\n",
       "        [0.5230],\n",
       "        [0.5698],\n",
       "        [0.5213],\n",
       "        [0.3789],\n",
       "        [0.5653],\n",
       "        [0.4731],\n",
       "        [0.4774],\n",
       "        [0.5847],\n",
       "        [0.5206],\n",
       "        [0.5149],\n",
       "        [0.4964],\n",
       "        [0.5083],\n",
       "        [0.5278],\n",
       "        [0.5836],\n",
       "        [0.5273],\n",
       "        [0.4921],\n",
       "        [0.5450],\n",
       "        [0.5768],\n",
       "        [0.5235],\n",
       "        [0.5314],\n",
       "        [0.5332],\n",
       "        [0.4502],\n",
       "        [0.5295],\n",
       "        [0.4151],\n",
       "        [0.5100],\n",
       "        [0.5446],\n",
       "        [0.6044],\n",
       "        [0.5790],\n",
       "        [0.5068],\n",
       "        [0.5760],\n",
       "        [0.5099],\n",
       "        [0.5808],\n",
       "        [0.4314],\n",
       "        [0.5588],\n",
       "        [0.5213],\n",
       "        [0.5812],\n",
       "        [0.5764],\n",
       "        [0.5618],\n",
       "        [0.5541],\n",
       "        [0.5315],\n",
       "        [0.6648],\n",
       "        [0.6261],\n",
       "        [0.5605],\n",
       "        [0.5159],\n",
       "        [0.5422],\n",
       "        [0.5424],\n",
       "        [0.5095],\n",
       "        [0.5737],\n",
       "        [0.4722],\n",
       "        [0.6132],\n",
       "        [0.4371],\n",
       "        [0.5754],\n",
       "        [0.5750],\n",
       "        [0.5140],\n",
       "        [0.5961],\n",
       "        [0.5538],\n",
       "        [0.4817],\n",
       "        [0.5006],\n",
       "        [0.4473],\n",
       "        [0.5128],\n",
       "        [0.5134],\n",
       "        [0.5062],\n",
       "        [0.4876],\n",
       "        [0.5371],\n",
       "        [0.4951],\n",
       "        [0.4113],\n",
       "        [0.5782],\n",
       "        [0.5581],\n",
       "        [0.3941],\n",
       "        [0.5559],\n",
       "        [0.5093],\n",
       "        [0.5595],\n",
       "        [0.5613],\n",
       "        [0.5659],\n",
       "        [0.5171],\n",
       "        [0.5066],\n",
       "        [0.5063],\n",
       "        [0.4302],\n",
       "        [0.4698],\n",
       "        [0.4403],\n",
       "        [0.5095],\n",
       "        [0.5302],\n",
       "        [0.4321],\n",
       "        [0.4705],\n",
       "        [0.5758],\n",
       "        [0.4415],\n",
       "        [0.5934],\n",
       "        [0.5749],\n",
       "        [0.4979],\n",
       "        [0.5210],\n",
       "        [0.5934],\n",
       "        [0.5710],\n",
       "        [0.5253],\n",
       "        [0.5805],\n",
       "        [0.4205],\n",
       "        [0.4550],\n",
       "        [0.5410],\n",
       "        [0.4966],\n",
       "        [0.5332],\n",
       "        [0.4621],\n",
       "        [0.5609],\n",
       "        [0.5242],\n",
       "        [0.4539],\n",
       "        [0.5569],\n",
       "        [0.5139],\n",
       "        [0.3880],\n",
       "        [0.5291],\n",
       "        [0.3973],\n",
       "        [0.4914],\n",
       "        [0.4378],\n",
       "        [0.5245],\n",
       "        [0.5115],\n",
       "        [0.5085],\n",
       "        [0.3855],\n",
       "        [0.5336],\n",
       "        [0.5537],\n",
       "        [0.5496],\n",
       "        [0.5793],\n",
       "        [0.5184],\n",
       "        [0.5387],\n",
       "        [0.5465],\n",
       "        [0.5892],\n",
       "        [0.5548],\n",
       "        [0.3815],\n",
       "        [0.5240],\n",
       "        [0.4413],\n",
       "        [0.4227],\n",
       "        [0.6025],\n",
       "        [0.4498],\n",
       "        [0.5159],\n",
       "        [0.5520],\n",
       "        [0.5826],\n",
       "        [0.4873],\n",
       "        [0.4710],\n",
       "        [0.5766],\n",
       "        [0.5366],\n",
       "        [0.5151],\n",
       "        [0.6258],\n",
       "        [0.4651],\n",
       "        [0.5493],\n",
       "        [0.5405],\n",
       "        [0.4486],\n",
       "        [0.4631],\n",
       "        [0.6318],\n",
       "        [0.5115],\n",
       "        [0.5069],\n",
       "        [0.4137],\n",
       "        [0.6376],\n",
       "        [0.4643],\n",
       "        [0.5326],\n",
       "        [0.5437],\n",
       "        [0.4282],\n",
       "        [0.4277],\n",
       "        [0.5515],\n",
       "        [0.5697],\n",
       "        [0.5474],\n",
       "        [0.4831],\n",
       "        [0.4923],\n",
       "        [0.4924],\n",
       "        [0.5665],\n",
       "        [0.6054],\n",
       "        [0.5531],\n",
       "        [0.5534],\n",
       "        [0.5612],\n",
       "        [0.5292],\n",
       "        [0.5804],\n",
       "        [0.5061],\n",
       "        [0.6449],\n",
       "        [0.5334],\n",
       "        [0.5440],\n",
       "        [0.5239],\n",
       "        [0.5774],\n",
       "        [0.5106],\n",
       "        [0.5033],\n",
       "        [0.5801],\n",
       "        [0.5561],\n",
       "        [0.5871],\n",
       "        [0.5154],\n",
       "        [0.5470],\n",
       "        [0.5090],\n",
       "        [0.5610],\n",
       "        [0.5473],\n",
       "        [0.4843],\n",
       "        [0.5869],\n",
       "        [0.5565],\n",
       "        [0.5039],\n",
       "        [0.5152],\n",
       "        [0.5228],\n",
       "        [0.5404],\n",
       "        [0.5143],\n",
       "        [0.5224],\n",
       "        [0.5405],\n",
       "        [0.3926],\n",
       "        [0.5230],\n",
       "        [0.4711],\n",
       "        [0.5018],\n",
       "        [0.5162],\n",
       "        [0.4776],\n",
       "        [0.5526],\n",
       "        [0.4678],\n",
       "        [0.4925],\n",
       "        [0.5817],\n",
       "        [0.5429],\n",
       "        [0.5543],\n",
       "        [0.4424],\n",
       "        [0.5422],\n",
       "        [0.5534],\n",
       "        [0.4373],\n",
       "        [0.5364],\n",
       "        [0.5287],\n",
       "        [0.5405],\n",
       "        [0.5422],\n",
       "        [0.5189],\n",
       "        [0.4629],\n",
       "        [0.5482],\n",
       "        [0.5644],\n",
       "        [0.5257],\n",
       "        [0.4347],\n",
       "        [0.4365],\n",
       "        [0.4661],\n",
       "        [0.5808],\n",
       "        [0.4686],\n",
       "        [0.5030],\n",
       "        [0.3950],\n",
       "        [0.4054],\n",
       "        [0.5287],\n",
       "        [0.5320],\n",
       "        [0.5731],\n",
       "        [0.4087],\n",
       "        [0.5715],\n",
       "        [0.5236],\n",
       "        [0.5523],\n",
       "        [0.5673],\n",
       "        [0.4791],\n",
       "        [0.5767],\n",
       "        [0.5541],\n",
       "        [0.5595],\n",
       "        [0.5560],\n",
       "        [0.4943],\n",
       "        [0.5151],\n",
       "        [0.5629],\n",
       "        [0.5177],\n",
       "        [0.5518],\n",
       "        [0.4774],\n",
       "        [0.5176],\n",
       "        [0.6024],\n",
       "        [0.4861],\n",
       "        [0.6108],\n",
       "        [0.5139],\n",
       "        [0.5142],\n",
       "        [0.5265],\n",
       "        [0.4796],\n",
       "        [0.6060],\n",
       "        [0.4707],\n",
       "        [0.5402],\n",
       "        [0.4801],\n",
       "        [0.5213],\n",
       "        [0.4962],\n",
       "        [0.5199],\n",
       "        [0.5265],\n",
       "        [0.5876],\n",
       "        [0.4654],\n",
       "        [0.5195],\n",
       "        [0.5152],\n",
       "        [0.4932],\n",
       "        [0.5604],\n",
       "        [0.5456],\n",
       "        [0.5244],\n",
       "        [0.5087],\n",
       "        [0.5346],\n",
       "        [0.5186],\n",
       "        [0.4595],\n",
       "        [0.6002],\n",
       "        [0.6074],\n",
       "        [0.5320],\n",
       "        [0.5527],\n",
       "        [0.5510],\n",
       "        [0.5858],\n",
       "        [0.4665],\n",
       "        [0.4705],\n",
       "        [0.4985],\n",
       "        [0.4899],\n",
       "        [0.5354],\n",
       "        [0.5770],\n",
       "        [0.4958],\n",
       "        [0.5777],\n",
       "        [0.5453],\n",
       "        [0.5921],\n",
       "        [0.5515],\n",
       "        [0.4939],\n",
       "        [0.5207],\n",
       "        [0.4650],\n",
       "        [0.5535],\n",
       "        [0.5939],\n",
       "        [0.4220],\n",
       "        [0.4599],\n",
       "        [0.5207],\n",
       "        [0.4157],\n",
       "        [0.4622],\n",
       "        [0.4664],\n",
       "        [0.4745],\n",
       "        [0.5118],\n",
       "        [0.5094],\n",
       "        [0.4623],\n",
       "        [0.5065],\n",
       "        [0.5169],\n",
       "        [0.5799],\n",
       "        [0.5303],\n",
       "        [0.5567],\n",
       "        [0.6286],\n",
       "        [0.5195]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72896283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model_3                                  [455, 1]                  --\n",
       "├─Sequential: 1-1                        [455, 1]                  --\n",
       "│    └─Linear: 2-1                       [455, 60]                 1,860\n",
       "│    └─ReLU: 2-2                         [455, 60]                 --\n",
       "│    └─Linear: 2-3                       [455, 1]                  61\n",
       "│    └─Sigmoid: 2-4                      [455, 1]                  --\n",
       "==========================================================================================\n",
       "Total params: 1,921\n",
       "Trainable params: 1,921\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.87\n",
       "==========================================================================================\n",
       "Input size (MB): 0.05\n",
       "Forward/backward pass size (MB): 0.22\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.28\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=model,input_size=X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f49d63e",
   "metadata": {},
   "source": [
    "### Actual Training Scenario "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b621a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.network=nn.Sequential(\n",
    "        nn.Linear(num_features, 60),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(60,1),\n",
    "        nn.Sigmoid()\n",
    "        )\n",
    "       \n",
    "\n",
    "    def forward(self, features):\n",
    "        # Convert input to float32 to match weight types\n",
    "        features = features.float() \n",
    "        out = self.network(features)\n",
    "        return out\n",
    "    \n",
    "    def loss_function(self, y_pred, y):\n",
    "        # Clamp predictions to avoid log(0)\n",
    "        epsilon = 1e-7\n",
    "        y_pred = torch.clamp(y_pred, epsilon, 1 - epsilon)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = -(y_train * torch.log(y_pred) + (1 - y_train) * torch.log(1 - y_pred)).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c92c099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model= Neural_Network(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6f2dbb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 0.6898123621940613\n",
      "Epoch 10: Loss 0.6714882254600525\n",
      "Epoch 20: Loss 0.6695539355278015\n",
      "Epoch 30: Loss 0.6690465211868286\n",
      "Epoch 40: Loss 0.6687359809875488\n",
      "Epoch 50: Loss 0.6684930920600891\n",
      "Epoch 60: Loss 0.6682928800582886\n",
      "Epoch 70: Loss 0.6681235432624817\n",
      "Epoch 80: Loss 0.6679782867431641\n",
      "Epoch 90: Loss 0.6678519248962402\n",
      "Epoch 100: Loss 0.6677409410476685\n",
      "Epoch 110: Loss 0.6676424145698547\n",
      "Epoch 120: Loss 0.6675544381141663\n",
      "Epoch 130: Loss 0.667475163936615\n",
      "Epoch 140: Loss 0.6674034595489502\n",
      "Epoch 150: Loss 0.6673383116722107\n",
      "Epoch 160: Loss 0.6672788858413696\n",
      "Epoch 170: Loss 0.6672245264053345\n",
      "Epoch 180: Loss 0.6671746373176575\n",
      "Epoch 190: Loss 0.6671288013458252\n",
      "Epoch 200: Loss 0.6670865416526794\n",
      "Epoch 210: Loss 0.6670475602149963\n",
      "Epoch 220: Loss 0.6670114398002625\n",
      "Epoch 230: Loss 0.6669780015945435\n",
      "Epoch 240: Loss 0.6669469475746155\n",
      "Epoch 250: Loss 0.6669180393218994\n",
      "Epoch 260: Loss 0.6668911576271057\n",
      "Epoch 270: Loss 0.6668660044670105\n",
      "Epoch 280: Loss 0.6668426990509033\n",
      "Epoch 290: Loss 0.6668208241462708\n",
      "Epoch 300: Loss 0.6668004393577576\n",
      "Epoch 310: Loss 0.6667812466621399\n",
      "Epoch 320: Loss 0.6667633056640625\n",
      "Epoch 330: Loss 0.6667464971542358\n",
      "Epoch 340: Loss 0.6667306423187256\n",
      "Epoch 350: Loss 0.666715681552887\n",
      "Epoch 360: Loss 0.6667015552520752\n",
      "Epoch 370: Loss 0.6666882634162903\n",
      "Epoch 380: Loss 0.6666758060455322\n",
      "Epoch 390: Loss 0.6666640639305115\n",
      "Epoch 400: Loss 0.6666528582572937\n",
      "Epoch 410: Loss 0.6666423082351685\n",
      "Epoch 420: Loss 0.6666322946548462\n",
      "Epoch 430: Loss 0.6666228175163269\n",
      "Epoch 440: Loss 0.666613757610321\n",
      "Epoch 450: Loss 0.6666052341461182\n",
      "Epoch 460: Loss 0.6665971875190735\n",
      "Epoch 470: Loss 0.6665893793106079\n",
      "Epoch 480: Loss 0.6665821075439453\n",
      "Epoch 490: Loss 0.6665750741958618\n",
      "Epoch 500: Loss 0.6665683388710022\n",
      "Epoch 510: Loss 0.666562020778656\n",
      "Epoch 520: Loss 0.6665558815002441\n",
      "Epoch 530: Loss 0.6665499806404114\n",
      "Epoch 540: Loss 0.6665443778038025\n",
      "Epoch 550: Loss 0.6665390729904175\n",
      "Epoch 560: Loss 0.6665339469909668\n",
      "Epoch 570: Loss 0.6665289402008057\n",
      "Epoch 580: Loss 0.6665241718292236\n",
      "Epoch 590: Loss 0.6665196418762207\n",
      "Epoch 600: Loss 0.6665152907371521\n",
      "Epoch 610: Loss 0.6665109992027283\n",
      "Epoch 620: Loss 0.6665068864822388\n",
      "Epoch 630: Loss 0.6665030121803284\n",
      "Epoch 640: Loss 0.6664993166923523\n",
      "Epoch 650: Loss 0.6664956212043762\n",
      "Epoch 660: Loss 0.6664921045303345\n",
      "Epoch 670: Loss 0.6664887070655823\n",
      "Epoch 680: Loss 0.6664854288101196\n",
      "Epoch 690: Loss 0.6664823293685913\n",
      "Epoch 700: Loss 0.666479229927063\n",
      "Epoch 710: Loss 0.6664761900901794\n",
      "Epoch 720: Loss 0.6664733290672302\n",
      "Epoch 730: Loss 0.6664705872535706\n",
      "Epoch 740: Loss 0.6664679050445557\n",
      "Epoch 750: Loss 0.6664652228355408\n",
      "Epoch 760: Loss 0.666462779045105\n",
      "Epoch 770: Loss 0.6664602160453796\n",
      "Epoch 780: Loss 0.6664578318595886\n",
      "Epoch 790: Loss 0.6664555668830872\n",
      "Epoch 800: Loss 0.6664533019065857\n",
      "Epoch 810: Loss 0.6664510369300842\n",
      "Epoch 820: Loss 0.6664489507675171\n",
      "Epoch 830: Loss 0.66644686460495\n",
      "Epoch 840: Loss 0.6664448976516724\n",
      "Epoch 850: Loss 0.6664428114891052\n",
      "Epoch 860: Loss 0.6664409637451172\n",
      "Epoch 870: Loss 0.6664390563964844\n",
      "Epoch 880: Loss 0.6664372682571411\n",
      "Epoch 890: Loss 0.6664354205131531\n",
      "Epoch 900: Loss 0.6664336919784546\n",
      "Epoch 910: Loss 0.6664320230484009\n",
      "Epoch 920: Loss 0.6664302945137024\n",
      "Epoch 930: Loss 0.6664287447929382\n",
      "Epoch 940: Loss 0.6664271354675293\n",
      "Epoch 950: Loss 0.6664256453514099\n",
      "Epoch 960: Loss 0.6664240956306458\n",
      "Epoch 970: Loss 0.6664226055145264\n",
      "Epoch 980: Loss 0.666421115398407\n",
      "Epoch 990: Loss 0.6664197444915771\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    # 1. Forward pass (calling the model directly is better than .forward())\n",
    "    y_pred = training_model(X_train)\n",
    "\n",
    "    # 2. Calculate loss\n",
    "    loss = training_model.loss_function(y_pred, y_train)\n",
    "\n",
    "    # 3. Backward pass (calculates gradients)\n",
    "    loss.backward()\n",
    "\n",
    "    # 4. Manual Gradient Descent\n",
    "    with torch.no_grad():\n",
    "        for param in training_model.parameters():\n",
    "            # Update the weights/biases\n",
    "            param -= learning_rate * param.grad\n",
    "            \n",
    "            # IMPORTANT: Manually zero the gradients after the update\n",
    "            # PyTorch accumulates gradients, so if you don't zero them, \n",
    "            # the next epoch's gradients will be added to these ones.\n",
    "            param.grad.zero_()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb461484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neural_Network(\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=30, out_features=60, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=60, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e42f1a",
   "metadata": {},
   "source": [
    "#### Using `loss function` and `optim function` given by Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2b272921",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network_2(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(num_features, 60),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(60, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        # We explicitly cast to .float() here\n",
    "        # This solves the \"Double vs Float\" mismatch by ensuring \n",
    "        # the input tensor matches the model's internal weights.\n",
    "        return self.network(features.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7687007",
   "metadata": {},
   "source": [
    "### Important Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4bbd02b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function=nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ba8c2faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_model=Neural_Network_2(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "48b80796",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.SGD(prod_model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d22bc78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 0.023803245276212692\n",
      "Epoch 10: Loss 0.023646414279937744\n",
      "Epoch 20: Loss 0.023492176085710526\n",
      "Epoch 30: Loss 0.023337949067354202\n",
      "Epoch 40: Loss 0.023186221718788147\n",
      "Epoch 50: Loss 0.023034701123833656\n",
      "Epoch 60: Loss 0.022884782403707504\n",
      "Epoch 70: Loss 0.022736111655831337\n",
      "Epoch 80: Loss 0.022591304033994675\n",
      "Epoch 90: Loss 0.022446751594543457\n",
      "Epoch 100: Loss 0.02230534888803959\n",
      "Epoch 110: Loss 0.022162558510899544\n",
      "Epoch 120: Loss 0.02202369086444378\n",
      "Epoch 130: Loss 0.021890126168727875\n",
      "Epoch 140: Loss 0.021756405010819435\n",
      "Epoch 150: Loss 0.02162381447851658\n",
      "Epoch 160: Loss 0.02149147167801857\n",
      "Epoch 170: Loss 0.021361716091632843\n",
      "Epoch 180: Loss 0.021233875304460526\n",
      "Epoch 190: Loss 0.021106602624058723\n",
      "Epoch 200: Loss 0.020980175584554672\n",
      "Epoch 210: Loss 0.020854251459240913\n",
      "Epoch 220: Loss 0.02073090150952339\n",
      "Epoch 230: Loss 0.02061009593307972\n",
      "Epoch 240: Loss 0.020485954359173775\n",
      "Epoch 250: Loss 0.02036723494529724\n",
      "Epoch 260: Loss 0.020246997475624084\n",
      "Epoch 270: Loss 0.020126948133111\n",
      "Epoch 280: Loss 0.020009012892842293\n",
      "Epoch 290: Loss 0.01989186555147171\n",
      "Epoch 300: Loss 0.01977609656751156\n",
      "Epoch 310: Loss 0.019660919904708862\n",
      "Epoch 320: Loss 0.019545942544937134\n",
      "Epoch 330: Loss 0.019432440400123596\n",
      "Epoch 340: Loss 0.01931988261640072\n",
      "Epoch 350: Loss 0.019208796322345734\n",
      "Epoch 360: Loss 0.019097937270998955\n",
      "Epoch 370: Loss 0.018987586721777916\n",
      "Epoch 380: Loss 0.018878497183322906\n",
      "Epoch 390: Loss 0.01877107471227646\n",
      "Epoch 400: Loss 0.01866420917212963\n",
      "Epoch 410: Loss 0.018556741997599602\n",
      "Epoch 420: Loss 0.018453558906912804\n",
      "Epoch 430: Loss 0.01834728568792343\n",
      "Epoch 440: Loss 0.018245602026581764\n",
      "Epoch 450: Loss 0.018142228946089745\n",
      "Epoch 460: Loss 0.01803959719836712\n",
      "Epoch 470: Loss 0.017938459292054176\n",
      "Epoch 480: Loss 0.017838958650827408\n",
      "Epoch 490: Loss 0.017739728093147278\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "\n",
    "    # Forward Pass\n",
    "    y_pred=prod_model(X_train)\n",
    "\n",
    "    # Loss Calculate\n",
    "    loss=loss_function(y_pred,y_train.view(-1,1).float())\n",
    "\n",
    "    # Clear Gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Loss Calculation\n",
    "    loss.backward()\n",
    "    \n",
    "    # Parameters Update\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8b85da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=training_model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac94c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.25%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): \n",
    "    \n",
    "    y_pred = prod_model(X_test)\n",
    "    \n",
    "    \n",
    "    y_pred_class = (y_pred > 0.6).float()\n",
    "    \n",
    "    \n",
    "    correct_predictions = (y_pred_class == y_test.view(-1, 1)).sum().item()\n",
    "    \n",
    "    \n",
    "    accuracy = (correct_predictions / y_test.size(0)) * 100\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae077fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8119b2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_Module_Learning (3.10.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
